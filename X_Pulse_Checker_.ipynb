{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDfg+ljU8ZZcW3hitzDpk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinath9121/X-Pulse-Checker/blob/main/X_Pulse_Checker_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96b5852e"
      },
      "source": [
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/X_logo_2023.svg/2560px-X_logo_2023.svg.png\" alt=\"X Logo\" width=\"300\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs your updated description with **Twitter replaced by X (formerly Twitter)**, reworded for clarity and professionalism:\n",
        "\n",
        "---\n",
        "\n",
        "This notebook provides a streamlined way to analyze trending conversations on **X (formerly Twitter)** related to any topic of interest. While the current setup focuses on posts from and about the city of Seattle, it can be easily adapted to other topics, regions, or entities.\n",
        "\n",
        "To speed up processing, especially for NLP tasks, enable GPU in Google Colab: go to **Runtime > Change runtime type**, select **GPU** under ‚ÄúHardware Accelerator,‚Äù and click **Save**.\n",
        "\n",
        "### What This Notebook Does:\n",
        "\n",
        "1. Scrapes recent posts from **X** related to your chosen topic using the X API.\n",
        "2. Extracts relevant entities using **Named Entity Recognition (NER)**.\n",
        "3. Performs **Sentiment Analysis** on the collected posts.\n",
        "4. Visualizes insights to understand the public pulse around the topic.\n",
        "\n",
        "The notebook uses `tweepy` for API access, `flair` for NER and sentiment tasks, and `seaborn` for plotting‚Äîall made fast and free through Google Colab‚Äôs GPU support.\n",
        "\n",
        "### NER (Named Entity Recognition):\n",
        "\n",
        "NER extracts categories like **Person**, **Organization**, or **Location** from text. For example, from ‚ÄúGeorge Washington went to Washington,‚Äù NER identifies ‚ÄúGeorge Washington‚Äù as a person and ‚ÄúWashington‚Äù as a location. This allows structured analysis of what people and places are being discussed.\n",
        "\n",
        "### Sentiment Analysis:\n",
        "\n",
        "This measures whether a statement is **positive**, **negative**, or **neutral**. For instance, ‚ÄúI hated this movie‚Äù would be negative, while ‚ÄúI loved this movie‚Äù would be positive. It‚Äôs a key tool for understanding public opinion.\n",
        "\n",
        "To run this project, you‚Äôll need developer credentials for X. Sign up at: [https://developer.twitter.com/en/apps](https://developer.twitter.com/en/apps)\n",
        "\n",
        "For advanced NLP models, see Flair‚Äôs GitHub tutorials: [https://github.com/zalandoresearch/flair](https://github.com/zalandoresearch/flair)\n"
      ],
      "metadata": {
        "id": "gQZhX6UE5EP0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52849a4d"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "*   [Authenticate with X API](#Authenticate-with-X-API)\n",
        "    *   [Enter Twitter Credentials](#Enter-Twitter-Credentials)\n",
        "    *   [Lets start scraping!](#Lets-start-scraping!)\n",
        "    *   [X Search API Inputs](#X-Search-API-Inputs)\n",
        "*   [Data Sciencing](#Data-Sciencing)\n",
        "    *   [Filter By Date Range](#Filter-By-Date-Range)\n",
        "    *   [NER and Sentiment Analysis](#NER-and-Sentiment-Analysis)\n",
        "*   [Visualize!](#Visualize!)\n",
        "    *   [Visualize Top TAGs](#Visualize-Top-TAGs)\n",
        "    *   [Get the Average Polarity Distribution](#Get-the-Average-Polarity-Distribution)\n",
        "    *   [Word Cloud](#Word-Cloud)\n",
        "    *   [Build Word Cloud For Top TAGs](#Build-Word-Cloud-For-Top-TAGs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get up and running, we need to import a bunch of stuff and install Flair. Run through the next 3 cells."
      ],
      "metadata": {
        "id": "NbCcf19d5YBl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGc4FbSqCJDg"
      },
      "source": [
        "# import lots of stuff\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "1t0tMqnBVpn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import Flair stuff\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger = SequenceTagger.load('ner')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "shLGWGSMlx7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import Flair Classifier\n",
        "from flair.models import TextClassifier\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KBu5zcP5l_iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authenticate with X API"
      ],
      "metadata": {
        "id": "EUWNUy2v5aIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter X Credentials"
      ],
      "metadata": {
        "id": "1JhnlUwo5ixp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- X API AUTHENTICATION CELL ---\n",
        "\n",
        "import tweepy\n",
        "\n",
        "# üîê Paste your Bearer Token here\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAADKN5QEAAAAASPTV%2FxGnVyi9FI1KB2pqBgMI63U%3DmI512JmpndTsLWSG7dUmHe6KvNVahGBsBSPQTuAWFZAyP63vSD\"\n",
        "\n",
        "try:\n",
        "    client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "    # Simple test to validate credentials\n",
        "    user_test = client.get_user(username=\"elonmusk\")\n",
        "    print(f\"‚úÖ Connected. Test user ID: {user_test.data.id}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Authentication failed:\", e)\n"
      ],
      "metadata": {
        "id": "JYBJFk1ottRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lets start scraping!**"
      ],
      "metadata": {
        "id": "SfinCN7a5ot2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### X Search API Inputs"
      ],
      "metadata": {
        "id": "nS--9qp55yG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# ----------------- SEARCH SETTINGS -----------------\n",
        "QUERY = \"AI -is:retweet lang:en\"  # Change this to your topic\n",
        "MAX_RESULTS = 30                 # Max allowed per request\n",
        "TOTAL_TWEETS = 30                # Total tweets to collect\n",
        "\n",
        "# ----------------- SCRAPE FUNCTION -----------------\n",
        "def fetch_tweets(query, max_results=30, total=30):\n",
        "    tweets = []\n",
        "    next_token = None\n",
        "    # Check if the 'client' object is defined from the authentication cell\n",
        "    if 'client' not in globals():\n",
        "        print(\"‚ùå Error: X API client not authenticated. Please run the authentication cell first.\")\n",
        "        return []\n",
        "\n",
        "    while len(tweets) < total:\n",
        "        try:\n",
        "            response = client.search_recent_tweets(\n",
        "                query=query,\n",
        "                tweet_fields=[\"created_at\", \"public_metrics\", \"lang\"],\n",
        "                max_results=max_results,\n",
        "                next_token=next_token\n",
        "            )\n",
        "            if response.data:\n",
        "                tweets.extend(response.data)\n",
        "                next_token = response.meta.get(\"next_token\")\n",
        "                print(f\"Fetched: {len(tweets)} tweets so far...\")\n",
        "                if not next_token:\n",
        "                    break\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(\"No data returned.\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå Error during scraping:\", e)\n",
        "            break\n",
        "    return tweets[:total]\n",
        "\n",
        "# ----------------- RUN SCRAPE -----------------\n",
        "raw_tweets = fetch_tweets(QUERY, MAX_RESULTS, TOTAL_TWEETS)\n",
        "\n",
        "# ----------------- STRUCTURE TO DATAFRAME -----------------\n",
        "def tweets_to_df(tweets):\n",
        "    data = []\n",
        "    for tweet in tweets:\n",
        "        metrics = tweet.public_metrics\n",
        "        data.append({\n",
        "            'id': tweet.id,\n",
        "            'created_at': tweet.created_at,\n",
        "            'text': tweet.text,\n",
        "            'like_count': metrics.get('like_count', 0),\n",
        "            'retweet_count': metrics.get('retweet_count', 0),\n",
        "            'reply_count': metrics.get('reply_count', 0),\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "df = tweets_to_df(raw_tweets)\n",
        "print(f\"\\n‚úÖ Collected {len(df)} tweets into DataFrame\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "hYtmtTZx0cwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Science"
      ],
      "metadata": {
        "id": "BIHBSuvQ55dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# load it into a pandas dataframe\n",
        "tweet_df = tweets_to_df(raw_tweets)\n",
        "tweet_df.to_csv('tweets.csv')\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "x1bShwIO5WT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538f4753"
      },
      "source": [
        "#@title Filter By Date Range\n",
        "today = datetime.now().date()\n",
        "yesterday = today - timedelta(1)\n",
        "\n",
        "start_dt = '' #@param {type:\"date\"}\n",
        "end_dt = '' #@param {type:\"date\"}\n",
        "\n",
        "if start_dt == '':\n",
        "  start_dt = yesterday\n",
        "else:\n",
        "  start_dt = datetime.strptime(start_dt, '%Y-%m-%d').date()\n",
        "\n",
        "if end_dt == '':\n",
        "  end_dt = today\n",
        "else:\n",
        "  end_dt = datetime.strptime(end_dt, '%Y-%m-%d').date()\n",
        "\n",
        "# Convert 'created_at' to datetime and then to date for comparison\n",
        "tweet_df['created_at'] = pd.to_datetime(tweet_df['created_at'])\n",
        "tweet_df['tweet_dt'] = tweet_df['created_at'].dt.date\n",
        "\n",
        "tweet_df = tweet_df[(tweet_df['tweet_dt'] >= start_dt)\n",
        "                    & (tweet_df['tweet_dt'] <= end_dt)]\n",
        "tweet_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER and Sentiment Analysis"
      ],
      "metadata": {
        "id": "95nXJGVi6JKZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cf41270"
      },
      "source": [
        "# predict NER and Sentiment Analysis\n",
        "nerlst = []\n",
        "\n",
        "for index, row in tqdm(tweet_df.iterrows(), total=tweet_df.shape[0]):\n",
        "  cleanedTweet = row['text'].replace(\"#\", \"\")\n",
        "  sentence = Sentence(cleanedTweet, use_tokenizer=True)\n",
        "\n",
        "  # predict NER tags\n",
        "  tagger.predict(sentence)\n",
        "\n",
        "  # get ner\n",
        "  ners = sentence.to_dict().get('entities', []) # Use .get() with a default empty list\n",
        "\n",
        "  # predict sentiment\n",
        "  classifier.predict(sentence)\n",
        "\n",
        "  label = sentence.labels[0]\n",
        "  response = {'result': label.value, 'polarity':label.score}\n",
        "\n",
        "  # get hashtags\n",
        "  hashtags = re.findall(r'#\\w+', row['text'])\n",
        "  if len(hashtags) >= 1:\n",
        "    for hashtag in hashtags:\n",
        "      ners.append({ 'type': 'Hashtag', 'text': hashtag })\n",
        "\n",
        "  for ner in ners:\n",
        "    adj_polarity = response['polarity']\n",
        "    if response['result'] == 'NEGATIVE':\n",
        "      adj_polarity = response['polarity'] * -1\n",
        "    try:\n",
        "      ner['type']\n",
        "    except:\n",
        "      ner['type'] = ''\n",
        "    nerlst.append([ row['tweet_dt'], row['id'], row['text'], ner.get('type', ''), ner.get('text', ''), response['result'], # Use .get() for ner type and text\n",
        "                   response['polarity'], adj_polarity, row['like_count'], row['reply_count'],\n",
        "                  row['retweet_count'] ])\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64c7a4d0"
      },
      "source": [
        "df_ner = pd.DataFrame(nerlst, columns=['tweet_dt', 'id', 'tweet', 'tag_type', 'tag', 'sentiment', 'polarity',\n",
        "                                       'adj_polarity','like_count', 'reply_count', 'retweet_count'])\n",
        "display(df_ner.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef1d96e7"
      },
      "source": [
        "# -------------------- Aggregation Per Tag --------------------\n",
        "\n",
        "# The banned_words list is included here for clarity, but filtering has already been done in a previous cell\n",
        "banned_words = ['Seattle', 'WA', '#Seattle', '#seattle', 'Washington', 'SEATTLE', 'WASHINGTON',\n",
        "                'seattle', 'Seattle WA', 'seattle wa','Seattle, WA', 'Seattle WA USA',\n",
        "                'Seattle, Washington', 'Seattle Washington', 'Wa', 'wa', '#Wa',\n",
        "                '#wa', '#washington', '#Washington', '#WA', '#PNW', '#pnw', '#northwest']\n",
        "\n",
        "# Filter out rows where the 'tag' is in the banned_words list\n",
        "df_ner_filtered = df_ner[~df_ner['tag'].isin(banned_words)].copy()\n",
        "\n",
        "\n",
        "# Group by tag and tag_type to calculate metrics using the filtered DataFrame\n",
        "grouped_df = df_ner_filtered.groupby(['tag', 'tag_type']).agg({\n",
        "    'adj_polarity': 'mean',\n",
        "    'like_count': 'sum',\n",
        "    'reply_count': 'sum',\n",
        "    'retweet_count': 'sum',\n",
        "    'tag': 'count'  # frequency\n",
        "}).rename(columns={'tag': 'Frequency'}).reset_index()\n",
        "\n",
        "# Assign sentiment category\n",
        "grouped_df['Sentiment'] = grouped_df['adj_polarity'].apply(lambda x: 'POSITIVE' if x >= 0 else 'NEGATIVE')\n",
        "\n",
        "# Show top 10 trending tags\n",
        "display(grouped_df.sort_values(by='Frequency', ascending=False).head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d11c616d"
      },
      "source": [
        "# Sort the grouped_df by Frequency in descending order\n",
        "sorted_grouped_df = grouped_df.sort_values(by='Frequency', ascending=False)\n",
        "display(sorted_grouped_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f610a4f"
      },
      "source": [
        "display(sorted_grouped_df[['tag', 'Frequency', 'adj_polarity']].head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31de7743"
      },
      "source": [
        "sorted_grouped_df = grouped_df.sort_values(by='Frequency', ascending=False)\n",
        "display(sorted_grouped_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculate Frequency, Likes, Replies, Retweets and Average Polarity per Tag.**"
      ],
      "metadata": {
        "id": "VRJ_ucDZ6PMW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc5219f6"
      },
      "source": [
        "# Create an overall Sentiment column based on the Average Polarity of the Tag.\n",
        "grouped_df['Sentiment'] = np.where(grouped_df['adj_polarity'] >= 0, 'POSITIVE', 'NEGATIVE')\n",
        "display(grouped_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e149683"
      },
      "source": [
        "#@title Visualize Top TAGs\n",
        "Filter_TAG = False #@param {type:\"boolean\"}\n",
        "TAG = 'Person' #@param [\"Hashtag\", \"Person\", \"Location\", \"Organization\"]\n",
        "#@markdown ###Pick how many tags to display per chart:\n",
        "Top_N = 10 #@param {type:\"integer\"}\n",
        "\n",
        "# get TAG value - The logic here needs to be adjusted based on the tag_type column in grouped_df\n",
        "# Let's assume the tag_type in grouped_df matches the dropdown values (Hashtag, Person, Location, Organization)\n",
        "# If TAG != 'Hashtag':\n",
        "#   TAG = TAG[:3].upper() # This logic is not necessary if tag_type column already contains full names\n",
        "\n",
        "\n",
        "if Filter_TAG:\n",
        "  # Filter by tag_type column which should match the TAG dropdown value\n",
        "  filtered_group = grouped_df[(grouped_df['tag_type'] == TAG)].copy()\n",
        "else:\n",
        "  filtered_group = grouped_df.copy() # Work on a copy to avoid modifying the original grouped_df\n",
        "\n",
        "\n",
        "# plot the figures\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.5)\n",
        "\n",
        "# Most Popular Tweets (based on Frequency)\n",
        "ax1 = fig.add_subplot(321)\n",
        "filtered_group_freq = filtered_group.sort_values(['Frequency'], ascending=False).head(Top_N)\n",
        "sns.barplot(x=\"Frequency\", y=\"tag\", data=filtered_group_freq, hue=\"Sentiment\", ax=ax1) # Specify ax=ax1\n",
        "\n",
        "# Most Liked Tweets\n",
        "ax2 = fig.add_subplot(322)\n",
        "filtered_group_likes = filtered_group.sort_values(['like_count'], ascending=False).head(Top_N) # Corrected column name\n",
        "sns.barplot(x=\"like_count\", y=\"tag\", data=filtered_group_likes, hue=\"Sentiment\", ax=ax2) # Corrected column name and specify ax=ax2\n",
        "\n",
        "# Most Replied Tweets\n",
        "ax3 = fig.add_subplot(323)\n",
        "filtered_group_replies = filtered_group.sort_values(['reply_count'], ascending=False).head(Top_N) # Corrected column name\n",
        "sns.barplot(x=\"reply_count\", y=\"tag\", data=filtered_group_replies, hue=\"Sentiment\", ax=ax3) # Corrected column name and specify ax=ax3\n",
        "\n",
        "# Most Retweeted Tweets\n",
        "ax4 = fig.add_subplot(324)\n",
        "filtered_group_retweets = filtered_group.sort_values(['retweet_count'], ascending=False).head(Top_N) # Corrected column name\n",
        "sns.barplot(x=\"retweet_count\", y=\"tag\", data=filtered_group_retweets, hue=\"Sentiment\", ax=ax4) # Corrected column name and specify ax=ax4\n",
        "\n",
        "\n",
        "ax1.title.set_text('Most Popular (Frequency)') # Added 'Frequency' for clarity\n",
        "ax2.title.set_text('Most Liked')\n",
        "ax3.title.set_text('Most Replied')\n",
        "ax4.title.set_text('Most Retweeted')\n",
        "\n",
        "ax1.set_ylabel('Tag') # Added y-label\n",
        "ax1.set_xlabel('Frequency') # Added x-label\n",
        "ax2.set_ylabel('Tag') # Added y-label\n",
        "ax2.set_xlabel('Total Likes') # Added x-label\n",
        "ax3.set_ylabel('Tag') # Added y-label\n",
        "ax3.set_xlabel('Total Replies') # Added x-label\n",
        "ax4.set_ylabel('Tag') # Added y-label\n",
        "ax4.set_xlabel('Total Retweets') # Added x-label\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
        "plt.show() # Ensure plot is displayed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get the Average Polarity Distribution.**"
      ],
      "metadata": {
        "id": "qTvXBzFX6X77"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a25613e"
      },
      "source": [
        "# Get the Average Polarity Distribution.\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Use sns.kdeplot for density plot as sns.distplot is deprecated\n",
        "# Use the correct DataFrame (filtered_group, which is already defined based on Filter_TAG and TAG)\n",
        "# Use the correct column name for average polarity ('adj_polarity')\n",
        "sns.kdeplot(data=filtered_group, x='adj_polarity', shade=True)\n",
        "\n",
        "plt.title('Distribution of Average Polarity per Tag') # Add a title for clarity\n",
        "plt.xlabel('Average Polarity') # Add x-label\n",
        "plt.ylabel('Density') # Add y-label\n",
        "plt.show() # Ensure the plot is displayed"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}